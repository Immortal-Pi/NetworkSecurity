{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1028fde",
   "metadata": {},
   "source": [
    "# data transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e80df104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants \n",
    "import numpy as np\n",
    "\n",
    "\"\"\" \n",
    "Data Transformation related constant start with DATA_TRANSFORMATION VAR NAME \n",
    "\"\"\"\n",
    "DATA_TRANSFORMATION_DIR_NAME: str = \"data_transformation\"\n",
    "DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR: str = 'transformed'\n",
    "DATA_TRANSFORMATION_TRANSFORMED_OBJECT_DIR: str = \"transformed_object\"\n",
    "PREPROCESSING_OBJECT_FILE_NAME: str = 'preprocessing.pkl'\n",
    "## KNN imputer to replace nan values \n",
    "DATA_TRANSFORMATION_IMPUTER_PARAMS: dict = {\n",
    "    'missing_Values': np.nan,\n",
    "    'n_neighbors': 3,\n",
    "    'weights': 'uniform'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2026b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact entity\n",
    "from dataclasses import dataclass \n",
    "\n",
    "@dataclass\n",
    "class DataTransformationArtifact:\n",
    "    transformed_object_file_path: str \n",
    "    transformed_train_file_path: str \n",
    "    transformed_test_file_path: str \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70f025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkSecurity\n",
      "Artifacts\n"
     ]
    }
   ],
   "source": [
    "# config \n",
    "from NetworkSecurity.entity.config_entity import TrainingPipelineConfig\n",
    "from NetworkSecurity.constants import training_pipeline\n",
    "import os \n",
    "\n",
    "\n",
    "class DataTransformationConfig:\n",
    "    def __init__(self, training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.data_transformation_dir: str = os.path.join(training_pipeline_config.artifact_dir, training_pipeline.DATA_TRANSFORMATION_DIR_NAME)\n",
    "        self.transformed_train_file_path: str = os.path.join(self.data_transformation_dir,training_pipeline.DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR,\n",
    "                                                                     training_pipeline.TRAIN_FILE_NAME.replace('csv','npy'),)\n",
    "        self.transformed_test_file_path: str = os.path.join(self.data_transformation_dir,training_pipeline.DATA_TRANSFORMATION_TRANSFORMED_DATA_DIR,\n",
    "                                                            training_pipeline.TEST_FILE_NAME.replace('csv','npy'),)\n",
    "        self.transformed_object_file_path: str = os.path.join(self.data_transformation_dir,training_pipeline.DATA_TRANSFORMATION_TRANSFORMED_OBJECT_DIR,\n",
    "                                                            training_pipeline.PREPROCESSING_OBJECT_FILE_NAME,)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1164f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component \n",
    "import sys \n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import  Pipeline\n",
    "\n",
    "from NetworkSecurity.constants.training_pipeline import TARGET_COLUMN\n",
    "from NetworkSecurity.constants.training_pipeline import DATA_TRANSFORMATION_IMPUTER_PARAMS\n",
    "\n",
    "from NetworkSecurity.entity.artifact_entity import (\n",
    "    DataTransformationArtifact,\n",
    "    DataValidationAritifact\n",
    ")\n",
    "\n",
    "# from NetworkSecurity.entity.config_entity import Data \n",
    "from NetworkSecurity.exception.exception import NetworkSecurityException\n",
    "from NetworkSecurity.logging.logger import logging\n",
    "from NetworkSecurity.utils.main_utils.utils import save_numpy_array_data,save_object\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, data_validation_artifact: DataValidationAritifact,\n",
    "                 data_transformation_config: DataTransformationConfig\n",
    "                 ):\n",
    "        try:\n",
    "            self.data_validation_artifact:DataValidationAritifact=data_validation_artifact\n",
    "            self.data_transformation_config:DataTransformationConfig=data_transformation_config\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_data(file_path)-> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    def get_data_transformer_object(cls)->Pipeline:\n",
    "        \"\"\" \n",
    "        It initialises a KKNN Imputer object with the parameters specified in the training_pipeline.py file\n",
    "        and returns a pipeline object with the KNNImputer object as the first step \n",
    "\n",
    "        Args:\n",
    "            cls: DataTransformation \n",
    "\n",
    "        Returns:\n",
    "            A pipeline object\n",
    "        \"\"\"\n",
    "        logging.info(\n",
    "            'Entered get_data_tranformer_object method of Transformation calass'\n",
    "        )\n",
    "        try:\n",
    "            imputer:KNNImputer=KNNImputer(**DATA_TRANSFORMATION_IMPUTER_PARAMS)\n",
    "            logging.info(f'Initialise KNNImputer with {DATA_TRANSFORMATION_IMPUTER_PARAMS}')\n",
    "            processor:Pipeline=Pipeline([('imputer',imputer)])\n",
    "            return processor\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "\n",
    "    def initiate_data_transformation(self)-> DataTransformationArtifact: \n",
    "        logging.info('Entered initiate_date_transformation method of DataTransformation class')\n",
    "        try:\n",
    "            logging.info('Starting data transformation')\n",
    "            train_df=DataTransformation.read_data(self.data_validation_artifact.valid_train_file_path)\n",
    "            test_df=DataTransformation.read_data(self.data_validation_artifact.valid_test_file_path)\n",
    "\n",
    "            # remove target variable\n",
    "            # training dataframe \n",
    "            input_feature_train_df=train_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            target_feature_train_df=train_df[TARGET_COLUMN]\n",
    "            target_feature_train_df=target_feature_train_df.replace(-1,0)\n",
    "\n",
    "            # testing dataframe \n",
    "            input_feature_test_df=test_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            target_feature_test_df=test_df[TARGET_COLUMN]\n",
    "            target_feature_test_df=target_feature_test_df.replace(-1,0)\n",
    "\n",
    "            # create preprocessor object \n",
    "            preprocessor=self.get_data_transformer_object()\n",
    "            preprocessor_object=preprocessor.fit(input_feature_train_df)\n",
    "            transformed_input_train_feature=preprocessor_object.transform(input_feature_train_df)\n",
    "            transformed_input_test_feature=preprocessor_object.transform(input_feature_test_df)\n",
    "\n",
    "            train_arr=np.c_[transformed_input_train_feature,np.array(target_feature_train_df)]\n",
    "            test_arr=np.c_[transformed_input_test_feature,np.array(target_feature_test_df)]\n",
    "\n",
    "            # save numpy array data \n",
    "            save_numpy_array_data(self.data_transformation_config.transformed_train_file_path, array=train_arr,)\n",
    "            save_numpy_array_data(self.data_transformation_config.transformed_test_file_path, array=test_arr,)\n",
    "            save_object(self.data_transformation_config.transformed_object_file_path, preprocessor_object, )\n",
    "\n",
    "            # preparing artifacts \n",
    "            data_transformation_artifact=DataTransformationArtifact(\n",
    "                transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                transformed_test_file_path=self.data_transformation_config.transformed_test_file_path\n",
    "            )\n",
    "            return data_transformation_artifact\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise NetworkSecurityException(e,sys)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "if __name__=='__main__':\n",
    "    try:\n",
    "        logging.info('Data Transformation config')\n",
    "        data_transformation_config=DataTransformationConfig(train)\n",
    "    except Exception as e:\n",
    "        raise NetworkSecurityException(e,sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
